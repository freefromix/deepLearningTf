{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan with tensorflow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial here: [Tutorial in 1.0](https://www.oreilly.com/learning/generative-adversarial-networks-for-beginners)\n",
    "\n",
    "![](img/GAN_Overall.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard.notebook extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard.notebook\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# !pip install -q tensorflow-gpu==2.0.0-beta0\n",
    "import tensorflow as tf\n",
    "\n",
    "# To generate GIFs\n",
    "# !pip install -q imageio\n",
    "tf.__version__\n",
    "\n",
    "%load_ext tensorboard.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "print(train_images.shape)\n",
    "\n",
    "# train_images.shape[0] = 60000\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "# Normalize the images to [-1, 1]\n",
    "train_images = (train_images - 127.5)/127.5\n",
    "\n",
    "BUFFER_SIZE = train_images.shape[0] # 60000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "The generator uses tf.keras.layers.Conv2DTranspose (upsampling) layers to produce an image from a seed (random noise).\n",
    "\n",
    "Start with a Dense layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1.\n",
    "\n",
    "Notice the tf.keras.layers.LeakyReLU activation for each layer, except the output layer which uses tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model=tf.keras.Sequential()\n",
    "    # units: Positive integer, dimensionality of the output space.\n",
    "    model.add(layers.Dense(units=7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1803183860>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "# Outputs random values from a normal distribution.\n",
    "noise = tf.random.normal(shape=[1, 100])\n",
    "assert noise.shape == (1, 100)\n",
    "\n",
    "generated_image = generator(noise, training=False)\n",
    "print(generated_image.shape)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriminator\n",
    "\n",
    "The discriminator is a CNN-based image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # filters: The number of output filters in the convolution).\n",
    "    # kernel_size: The height and width of the 2D convolution window.\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size=(5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    # rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "    # `rate` of input units to 0 at each update during training time, which helps prevent overfitting\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=128, kernel_size=(5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n",
      "tf.Tensor([[-0.00052168]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(generated_image.shape)\n",
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss and optimizers\n",
    "\n",
    "**tf.keras.losses.BinaryCrossentropy()**: Computes the cross-entropy loss between true labels and predicted labels.\n",
    "\n",
    "Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction.\n",
    "\n",
    "from_logits: Whether to interpret `y_pred` as a tensor of [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we assume that `y_pred` contains probabilities (i.e., values in [0, 1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator loss\n",
    "\n",
    "This method quantifies how well the discriminator is able to distinguish real images from fakes.\n",
    "\n",
    "It compares:\n",
    "\n",
    "- the discriminator's predictions on real images to an array of 1s\n",
    "- and the discriminator's predictions on fake (generated) images to an array of 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    # tf.ones_like(): this operation returns a tensor of the same type and shape as `tensor` with all elements set to 1.\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator loss\n",
    "\n",
    "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, we will compare the discriminators decisions on the generated images to an array of 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The discriminator and the generator optimizers are different since we will train two networks separately!!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image.\n",
    "\n",
    "The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator).\n",
    "\n",
    "The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "# A tf.function you define is just like a core TensorFlow operation: You can execute it eagerly; you can use it in a graph; it has gradients; and so on.\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        # Produce images for the GIF as we go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 1, seed)\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator, epochs, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Call the train() method defined above to train the generator and discriminator simultaneously. Note, training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate).\n",
    "\n",
    "At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits. This may take about one minute / epoch with the default settings on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-afb5732130e2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Produce images for the GIF as we go\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    402\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \"\"\"\n\u001b[1;32m    588\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 589\u001b[0;31m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[1;32m    590\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    591\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f180047e908>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "    return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'image_at_epoch_0050.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-7d0477526f8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-dbd0e0e02b23>\u001b[0m in \u001b[0;36mdisplay_image\u001b[0;34m(epoch_no)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display a single image using the epoch number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image_at_epoch_{:04d}.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'image_at_epoch_0050.png'"
     ]
    }
   ],
   "source": [
    "display_image(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an animated gif\n",
    "\n",
    "Use imageio to create an animated gif using the images saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('image*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    last = -1\n",
    "    for i,filename in enumerate(filenames):\n",
    "        frame = 2*(i**0.5)\n",
    "        if round(frame) > round(last):\n",
    "            last = frame\n",
    "        else:\n",
    "            continue\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "\n",
    "import IPython\n",
    "if IPython.version_info > (6,2,0,''):\n",
    "    display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download(anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
